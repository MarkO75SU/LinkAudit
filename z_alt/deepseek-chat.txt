erstelle den code. secrects kommen in eine .env:
ðŸ”— LinkAudit â€“ KI-gestÃ¼tzte Linkanalyse fÃ¼r Transparenz & Wirkung

ðŸŽ¯ Zweck
LinkAudit hilft Nutzern, die Herkunft, emotionale Wirkung und VertrauenswÃ¼rdigkeit von digitalen Links zu verstehen. Es macht sichtbar, was hinter einem Link steckt â€“ technisch, semantisch und psychologisch.

---

ðŸ§  Funktionen

| Modul               | Beschreibung                                                                 |
|---------------------|-------------------------------------------------------------------------------|
| Linkeingabe     | Nutzer gibt einen beliebigen Link ein (z.â€¯B. aus Social Media, News, Werbung) |
| Crawler         | Holt HTML, Metadaten, Redirects, Tracking-Parameter                           |
| Inhaltsanalyse  | KI bewertet Titel & Meta-Description auf TonalitÃ¤t, Framing, Emotion          |
| Reputation      | WHOIS, Domainalter, Trust-Scores, Kontextvernetzung                           |
| ImpactRadar     | Visualisiert Wirkung: Vertrauen, Polarisierung, ViralitÃ¤t, Bias               |
| Export          | JSON, PDF, QR-Code, Embed-Widget                                              |

---

ðŸ§ª MVP mit Python (lokal ausfÃ¼hrbar)

- Tools: requests, beautifulsoup4, streamlit, openai oder google.generativeai, matplotlib  
- Kein Hosting nÃ¶tig, keine Datenbank erforderlich  
- 400â€“700 Zeilen Code fÃ¼r Basisversion  
- Analyse nur von Titel & Meta spart Tokens und Kosten

---

ðŸ’° Kosten & Skalierung

| Phase       | Kosten (ca.) | Tools                        |
|-------------|--------------|------------------------------|
| MVP lokal | â‚¬0â€“5/Monat   | Gemini CLI, Streamlit, OpenPageRank |
| Web-App  | â‚¬20â€“50/Monat | Supabase, Vercel, Render     |
| API-Modus| â‚¬50+/Monat   | bei hohem Traffic oder Teamnutzung |

---

ðŸŽ¯ Zielgruppen

- Journalisten & Redaktionen  
- NGOs & Bildungsprojekte  
- LehrkrÃ¤fte & MedienpÃ¤dagogen  
- Plattformen mit Kommentar-Moderation  
- Kritische Privatnutzer

---

ðŸ§± Architektur (High-Level)

`
User â†’ Streamlit UI â†’ analyze_link.py:
   â”œâ”€â”€ Crawler: requests + bs4
   â”œâ”€â”€ Redirects: urllib
   â”œâ”€â”€ KI-Auswertung: Gemini/OpenAI
   â”œâ”€â”€ Reputation: WHOIS + OpenPageRank
   â””â”€â”€ Visualisierung: matplotlib
â†’ Ausgabe: JSON + Chart
`

---


---

âœ… Was geht mit Python?

| Funktion                     | Python-Tool/Libs                     | Status       |
|-----------------------------|--------------------------------------|--------------|
| Linkeingabe & UI         | streamlit, gradio, flask       | âœ… Einfach   |
| HTML-Crawling            | requests, beautifulsoup4, httpx| âœ… Stabil    |
| Redirect-Analyse         | requests, urllib, tldextract   | âœ… MÃ¶glich   |
| Meta-Parsing             | bs4, lxml, html.parser         | âœ… Schnell   |
| KI-Auswertung (Text)     | openai, google.generativeai, transformers | âœ… Modular |
| Sentiment & Emotion      | textblob, vaderSentiment, transformers | âœ… Lokal oder API |
| Reputation & WHOIS       | python-whois, dnspython, openpagerank | âœ… Teilweise |
| Visualisierung           | matplotlib, plotly, networkx   | âœ… Lokal     |
| Export (PDF/JSON)        | pdfkit, json, reportlab        | âœ… Optional  |

---

ðŸ§± Technischer Ablauf in Python

1. User gibt Link ein â†’ streamlit.text_input()  
2. Crawler holt HTML â†’ requests.get()  
3. Parser extrahiert Titel & Meta â†’ BeautifulSoup()  
4. KI-Modul bewertet Inhalt â†’ openai.ChatCompletion() oder gemini.generate_content()  
5. Ergebnis wird visualisiert â†’ plotly oder matplotlib  
6. Export als JSON oder PDF â†’ json.dump() oder pdfkit.from_string()

---

ðŸ§ª Vorteile von Python

- Lokal ausfÃ¼hrbar ohne Hosting  
- GroÃŸe Auswahl an NLP- und KI-Bibliotheken  
- Einfach zu testen, debuggen und erweitern  
- Ideal fÃ¼r Prototyping und wissenschaftliche Analyse  
- Gut kombinierbar mit Gemini CLI oder OpenAI API

Hier ist der vollstÃ¤ndige Code fÃ¼r LinkAudit. Erstelle eine .env Datei fÃ¼r die Secrets:
.env Datei
env

GOOGLE_API_KEY=dein_google_gemini_api_key_here
OPENPAGERANK_API_KEY=dein_openpagerank_api_key_here

requirements.txt
txt

streamlit==1.28.0
requests==2.31.0
beautifulsoup4==4.12.2
python-dotenv==1.0.0
google-generativeai==0.3.0
matplotlib==3.7.2
plotly==5.15.0
python-whois==0.8.0
tldextract==3.4.4
urllib3==1.26.16
pdfkit==1.0.0

main.py
python

import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from urllib.parse import urlparse
import whois
import tldextract
from datetime import datetime
import google.generativeai as genai
import os
from dotenv import load_dotenv
import pdfkit
import time

# Load environment variables
load_dotenv()

class LinkAudit:
    def __init__(self):
        self.setup_gemini()
        
    def setup_gemini(self):
        """Initialize Gemini AI"""
        api_key = os.getenv('GOOGLE_API_KEY')
        if api_key:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-pro')
        else:
            st.error("Google API Key nicht gefunden. Bitte in .env Datei hinterlegen.")
            self.model = None

    def crawl_url(self, url):
        """Crawl URL and extract basic information"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract basic information
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "Kein Titel gefunden"
            
            # Extract meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '').strip() if meta_desc else ""
            
            # Extract other meta tags
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            keywords = meta_keywords.get('content', '').strip() if meta_keywords else ""
            
            # Extract redirect history
            redirect_history = [str(resp.url) for resp in response.history] + [response.url]
            
            return {
                'success': True,
                'title': title_text,
                'description': description,
                'keywords': keywords,
                'final_url': response.url,
                'redirect_history': redirect_history,
                'status_code': response.status_code,
                'content_length': len(response.content),
                'html_content': response.text
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def analyze_content_with_ai(self, title, description):
        """Analyze content using Gemini AI"""
        if not self.model:
            return {"error": "Gemini AI nicht konfiguriert"}
            
        try:
            prompt = f"""
            Analysiere diesen Webseiten-Inhalt auf emotionale Wirkung, TonalitÃ¤t und Framing:

            Titel: {title}
            Beschreibung: {description}

            Bewerte in folgenden Kategorien (1-10, wobei 1=sehr negativ/neutral, 10=sehr positiv/emotional):
            - Emotionale IntensitÃ¤t
            - VertrauenswÃ¼rdigkeit
            - Polarisierungspotential
            - ObjektivitÃ¤t/SubjektivitÃ¤t
            - ViralitÃ¤tspotential

            Gib eine kurze Zusammenfassung der emotionalen Wirkung und antworte im JSON Format:
            {{
                "emotional_intensity": score,
                "trust_score": score,
                "polarization_potential": score,
                "objectivity_score": score,
                "virality_potential": score,
                "summary": "kurze Zusammenfassung",
                "framing_analysis": "Beschreibung des Framings"
            }}
            """
            
            response = self.model.generate_content(prompt)
            # Parse JSON from response
            import re
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {"error": "Konnte AI-Antwort nicht parsen"}
                
        except Exception as e:
            return {"error": f"AI-Analyse fehlgeschlagen: {str(e)}"}

    def get_domain_reputation(self, url):
        """Analyze domain reputation"""
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            
            # WHOIS information
            whois_info = whois.whois(domain)
            
            # Domain age
            creation_date = whois_info.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]
                
            domain_age = None
            if creation_date:
                domain_age = (datetime.now() - creation_date).days
                
            # OpenPageRank API
            page_rank = self.get_page_rank(domain)
            
            # TLD analysis
            extracted = tldextract.extract(url)
            tld = extracted.suffix
            
            return {
                'domain': domain,
                'tld': tld,
                'creation_date': str(creation_date) if creation_date else "Unbekannt",
                'domain_age_days': domain_age,
                'page_rank': page_rank,
                'registrar': whois_info.registrar,
                'country': whois_info.country
            }
            
        except Exception as e:
            return {
                'domain': domain,
                'error': str(e)
            }

    def get_page_rank(self, domain):
        """Get PageRank from OpenPageRank API"""
        try:
            api_key = os.getenv('OPENPAGERANK_API_KEY')
            if not api_key:
                return "API Key nicht konfiguriert"
                
            headers = {'API-OPR': api_key}
            response = requests.get(
                f'https://openpagerank.com/api/v1.0/getPageRank?domains[]={domain}',
                headers=headers
            )
            
            if response.status_code == 200:
                data = response.json()
                if data['response']:
                    return data['response'][0].get('page_rank_decimal', 'N/A')
                    
            return "N/A"
        except:
            return "N/A"

    def analyze_tracking_parameters(self, url):
        """Analyze URL for tracking parameters"""
        parsed_url = urlparse(url)
        query_params = parsed_url.query
        
        tracking_indicators = ['utm_', 'fbclid', 'gclid', 'msclkid', 'trk_', 'ref_']
        found_params = []
        
        if query_params:
            for param in query_params.split('&'):
                if any(indicator in param.lower() for indicator in tracking_indicators):
                    found_params.append(param)
                    
        return {
            'has_tracking_params': len(found_params) > 0,
            'tracking_params': found_params,
            'total_params': len(query_params.split('&')) if query_params else 0
        }

def create_radar_chart(analysis_data):
    """Create radar chart for impact analysis"""
    categories = ['Emotionale IntensitÃ¤t', 'VertrauenswÃ¼rdigkeit', 
                  'Polarisierung', 'ObjektivitÃ¤t', 'ViralitÃ¤t']
    
    scores = [
        analysis_data.get('emotional_intensity', 5),
        analysis_data.get('trust_score', 5),
        analysis_data.get('polarization_potential', 5),
        analysis_data.get('objectivity_score', 5),
        analysis_data.get('virality_potential', 5)
    ]
    
    fig = go.Figure(data=go.Scatterpolar(
        r=scores,
        theta=categories,
        fill='toself',
        line=dict(color='blue')
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 10]
            )),
        showlegend=False,
        title="Impact Radar - Wirkungsanalyse"
    )
    
    return fig

def export_as_pdf(audit_results):
    """Export audit results as PDF"""
    html_content = f"""
    <html>
    <head>
        <title>LinkAudit Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .header {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
            .section {{ margin: 20px 0; }}
            .score {{ color: #e74c3c; font-weight: bold; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>LinkAudit Report</h1>
            <p>Erstellt am: {datetime.now().strftime("%d.%m.%Y %H:%M")}</p>
        </div>
        
        <div class="section">
            <h2>Analyse fÃ¼r: {audit_results['url']}</h2>
            <p><strong>Titel:</strong> {audit_results.get('title', 'N/A')}</p>
            <p><strong>Finale URL:</strong> {audit_results.get('final_url', 'N/A')}</p>
        </div>
        
        <div class="section">
            <h2>KI-Analyse</h2>
            <p><strong>Emotionale IntensitÃ¤t:</strong> <span class="score">{audit_results.get('emotional_intensity', 'N/A')}</span></p>
            <p><strong>VertrauenswÃ¼rdigkeit:</strong> <span class="score">{audit_results.get('trust_score', 'N/A')}</span></p>
            <p><strong>Zusammenfassung:</strong> {audit_results.get('summary', 'N/A')}</p>
        </div>
        
        <div class="section">
            <h2>Domain-Reputation</h2>
            <p><strong>Domain:</strong> {audit_results.get('domain', 'N/A')}</p>
            <p><strong>Domain-Alter:</strong> {audit_results.get('domain_age_days', 'N/A')} Tage</p>
            <p><strong>PageRank:</strong> {audit_results.get('page_rank', 'N/A')}</p>
        </div>
    </body>
    </html>
    """
    
    # You might need to install wkhtmltopdf and specify the path
    # config = pdfkit.configuration(wkhtmltopdf='/path/to/wkhtmltopdf')
    try:
        pdf = pdfkit.from_string(html_content, False)
        return pdf
    except:
        return None

def main():
    st.set_page_config(
        page_title="LinkAudit - KI-gestÃ¼tzte Linkanalyse",
        page_icon="ðŸ”—",
        layout="wide"
    )
    
    st.title("ðŸ”— LinkAudit")
    st.subheader("KI-gestÃ¼tzte Linkanalyse fÃ¼r Transparenz & Wirkung")
    
    # Initialize auditor
    auditor = LinkAudit()
    
    # URL input
    url = st.text_input("Gib einen Link zur Analyse ein:", placeholder="https://example.com")
    
    if st.button("Link analysieren") and url:
        with st.spinner("Analysiere Link..."):
            # Perform all analyses
            crawl_results = auditor.crawl_url(url)
            
            if not crawl_results['success']:
                st.error(f"Fehler beim Crawlen: {crawl_results['error']}")
                return
            
            # AI Analysis
            ai_analysis = auditor.analyze_content_with_ai(
                crawl_results['title'], 
                crawl_results['description']
            )
            
            # Domain reputation
            domain_analysis = auditor.get_domain_reputation(url)
            
            # Tracking analysis
            tracking_analysis = auditor.analyze_tracking_parameters(url)
            
            # Combine all results
            audit_results = {
                **crawl_results,
                **ai_analysis,
                **domain_analysis,
                **tracking_analysis,
                'url': url,
                'analysis_date': datetime.now().isoformat()
            }
            
            # Display results
            col1, col2 = st.columns(2)
            
            with col1:
                st.header("ðŸ“Š Basis-Informationen")
                st.write(f"**Titel:** {crawl_results['title']}")
                st.write(f"**Beschreibung:** {crawl_results['description'] or 'Keine Beschreibung gefunden'}")
                st.write(f"**Finale URL:** {crawl_results['final_url']}")
                st.write(f"**Status Code:** {crawl_results['status_code']}")
                
                if crawl_results['redirect_history'] and len(crawl_results['redirect_history']) > 1:
                    st.write("**Redirects:**")
                    for i, redirect in enumerate(crawl_results['redirect_history']):
                        st.write(f"  {i+1}. {redirect}")
            
            with col2:
                st.header("ðŸŽ¯ KI-Analyse")
                if 'error' not in ai_analysis:
                    st.metric("Emotionale IntensitÃ¤t", ai_analysis.get('emotional_intensity', 'N/A'))
                    st.metric("VertrauenswÃ¼rdigkeit", ai_analysis.get('trust_score', 'N/A'))
                    st.metric("Polarisierung", ai_analysis.get('polarization_potential', 'N/A'))
                    st.metric("ObjektivitÃ¤t", ai_analysis.get('objectivity_score', 'N/A'))
                    st.metric("ViralitÃ¤t", ai_analysis.get('virality_potential', 'N/A'))
                    
                    st.write("**Zusammenfassung:**", ai_analysis.get('summary', 'N/A'))
                    st.write("**Framing-Analyse:**", ai_analysis.get('framing_analysis', 'N/A'))
                else:
                    st.error("KI-Analyse fehlgeschlagen")
            
            # Radar chart
            if 'error' not in ai_analysis:
                st.plotly_chart(create_radar_chart(ai_analysis), use_container_width=True)
            
            # Domain Information
            st.header("ðŸŒ Domain-Reputation")
            col3, col4 = st.columns(2)
            
            with col3:
                st.write(f"**Domain:** {domain_analysis.get('domain', 'N/A')}")
                st.write(f"**TLD:** {domain_analysis.get('tld', 'N/A')}")
                st.write(f"**Registrar:** {domain_analysis.get('registrar', 'N/A')}")
                
            with col4:
                st.write(f"**Erstellungsdatum:** {domain_analysis.get('creation_date', 'N/A')}")
                st.write(f"**Domain-Alter:** {domain_analysis.get('domain_age_days', 'N/A')} Tage")
                st.write(f"**PageRank:** {domain_analysis.get('page_rank', 'N/A')}")
            
            # Tracking Analysis
            st.header("ðŸ” Tracking-Analyse")
            if tracking_analysis['has_tracking_params']:
                st.warning(f"Tracking-Parameter gefunden: {len(tracking_analysis['tracking_params'])}")
                for param in tracking_analysis['tracking_params']:
                    st.code(param)
            else:
                st.success("Keine Tracking-Parameter erkannt")
            
            # Export options
            st.header("ðŸ“¤ Export")
            col5, col6, col7 = st.columns(3)
            
            with col5:
                if st.button("JSON exportieren"):
                    st.download_button(
                        label="JSON herunterladen",
                        data=json.dumps(audit_results, indent=2, ensure_ascii=False),
                        file_name=f"linkaudit_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                        mime="application/json"
                    )
            
            with col6:
                if st.button("PDF exportieren"):
                    pdf_data = export_as_pdf(audit_results)
                    if pdf_data:
                        st.download_button(
                            label="PDF herunterladen",
                            data=pdf_data,
                            file_name=f"linkaudit_{datetime.now().strftime('%Y%m%d_%H%M')}.pdf",
                            mime="application/pdf"
                        )
                    else:
                        st.error("PDF-Export fehlgeschlagen")
            
            with col7:
                # QR Code would require additional dependencies
                st.info("QR-Code Funktion benÃ¶tigt zusÃ¤tzliche Bibliotheken")
    
    # Sidebar with information
    with st.sidebar:
        st.header("â„¹ï¸ Ãœber LinkAudit")
        st.write("""
        LinkAudit analysiert Links auf:
        - **Emotionale Wirkung**
        - **VertrauenswÃ¼rdigkeit**  
        - **Domain-Reputation**
        - **Tracking-Parameter**
        - **Redirect-Verhalten**
        """)
        
        st.header("ðŸŽ¯ Zielgruppen")
        st.write("""
        - Journalisten & Redaktionen
        - NGOs & Bildungsprojekte
        - LehrkrÃ¤fte & MedienpÃ¤dagogen
        - Kritische Privatnutzer
        """)
        
        st.header("âš™ï¸ Konfiguration")
        st.write("API Keys in .env Datei benÃ¶tigt:")
        st.code("""
GOOGLE_API_KEY=dein_gemini_key
OPENPAGERANK_API_KEY=dein_pagerank_key
        """)

if __name__ == "__main__":
    main()

Installation und AusfÃ¼hrung

    Umgebung einrichten:

bash

python -m venv linkaudit-env
source linkaudit-env/bin/activate  # Windows: linkaudit-env\Scripts\activate

    AbhÃ¤ngigkeiten installieren:

bash

pip install -r requirements.txt

    FÃ¼r PDF-Export (optional):

bash

# Ubuntu/Debian
sudo apt-get install wkhtmltopdf

# macOS
brew install wkhtmltopdf

# Windows: Download von https://wkhtmltopdf.org/

    App starten:

bash

streamlit run main.py

Funktionen des Codes

    âœ… Link-Crawling mit Requests & BeautifulSoup

    âœ… KI-Analyse mit Google Gemini

    âœ… Domain-Reputation mit WHOIS & OpenPageRank

    âœ… Tracking-Parameter Erkennung

    âœ… Visualisierung mit Plotly Radar-Charts

    âœ… Export als JSON und PDF

    âœ… Streamlit UI fÃ¼r benutzerfreundliche OberflÃ¤che

Der Code ist modular aufgebaut und kann einfach um weitere Analyse-Features erweitert werden.
Hier ist die vollstÃ¤ndige, strukturierte Zusammenfassung von LinkAudit â€“ deinem modularen, KI-gestÃ¼tzten Tool zur Linkanalyse. Sie enthÃ¤lt Idee, Nutzen, Zielgruppe, Funktionen, Technik, MVP-Umsetzung, Kosten und Erweiterungspotenzial:

---

ðŸ”— LinkAudit â€“ Transparenz fÃ¼r digitale Links

ðŸŽ¯ Zweck
LinkAudit analysiert digitale Links (z.â€¯B. aus Social Media, News, Werbung) und bewertet sie hinsichtlich:
- Herkunft & Redirects
- Emotionale Wirkung & Framing
- Semantik & Bias
- Reputation & VertrauenswÃ¼rdigkeit
- Algorithmischer VerstÃ¤rkung (z.â€¯B. ViralitÃ¤t, Clickbait)

Ziel: Medienkompetenz stÃ¤rken, kritisches Denken fÃ¶rdern, digitale Verantwortung sichtbar machen.

---

ðŸ‘¥ Zielgruppen

| Gruppe             | Nutzen                                 |
|--------------------|----------------------------------------|
| Journalisten       | QuellenprÃ¼fung, Bias-Erkennung         |
| NGOs               | Link-Transparenz in Kampagnen          |
| LehrkrÃ¤fte         | Medienkompetenz-Training                |
| Privatnutzer       | â€žWas steckt hinter diesem Link?â€œ       |
| Plattformen        | API zur Linkbewertung in Kommentaren   |

---

ðŸ§© Funktionen (MVP)

| Funktion                     | Status       | Tools/Libs (Python)                        |
|------------------------------|--------------|--------------------------------------------|
| âœ… Link-Crawling             | âœ”ï¸ Fertig     | requests, beautifulsoup4               |
| âœ… Redirect-Erkennung        | âœ”ï¸ Fertig     | urllib, httpx                          |
| âœ… Tracking-Parameter-Check  | âœ”ï¸ Fertig     | urlparse, manuell                        |
| âœ… KI-Analyse (Emotion, Ton) | âœ”ï¸ Fertig     | google.generativeai, openai, textblob|
| âœ… Domain-Reputation         | âœ”ï¸ Fertig     | python-whois, OpenPageRank API         |
| âœ… Visualisierung            | âœ”ï¸ Fertig     | plotly, matplotlib                     |
| âœ… Export (JSON, PDF)        | âœ”ï¸ Fertig     | json, pdfkit, reportlab              |
| âœ… Streamlit UI              | âœ”ï¸ Fertig     | streamlit                                |

---

ðŸ§ª MVP-Umsetzung

- Sprache: Python  
- UI: Streamlit (lokal ausfÃ¼hrbar)  
- Codeumfang: ca. 400â€“700 Zeilen  
- Speicherung: lokal, keine Datenbank nÃ¶tig  
- KI-Analyse: nur Titel + Meta â†’ tokenoptimiert  
- Visualisierung: Impact-Radar als Spider-Chart

---

ðŸ’° KostenÃ¼bersicht

| Phase       | Tools/Services         | Kosten (ca.)     |
|-------------|------------------------|------------------|
| MVP lokal   | Gemini CLI, Streamlit  | â‚¬0â€“5/Monat       |
| Web-App     | Supabase, Vercel       | â‚¬20â€“50/Monat     |
| API-Modus   | Gemini API, Render     | â‚¬50+/Monat       |

> Free-Tier-fÃ¤hig mit Supabase, Gemini CLI, OpenPageRank, WHOIS

---

ðŸ”§ Erweiterungspotenzial

- ðŸ” Rate-Limiting & Caching  
- ðŸ§  Framing-Analyse & Bias-Erkennung  
- ðŸ“Š Quellenkarte (Verbreitung)  
- ðŸ“¤ Embed-Widget & QR-Code  
- ðŸ§± Nutzerverwaltung & API-Zugang  
- ðŸ§  KI-Empfehlungen (â€žMÃ¶chtest du Alternativen sehen?â€œ)

---

ðŸ§± Architektur (High-Level)

`
User â†’ Streamlit UI â†’ analyze_link.py:
   â”œâ”€â”€ Crawler: requests + bs4
   â”œâ”€â”€ Redirects: urllib
   â”œâ”€â”€ KI-Auswertung: Gemini/OpenAI
   â”œâ”€â”€ Reputation: WHOIS + OpenPageRank
   â””â”€â”€ Visualisierung: plotly
â†’ Ausgabe: JSON + Chart
`

---

Wenn du willst, kann ich dir jetzt direkt das Starter-Skript analyze_link.py, ein Prompt-Template fÃ¼r die KI-Auswertung oder ein Launch-Backlog mit Priorisierung liefern. Du bist bereit fÃ¼r den ersten echten Prototyp. ja.

Perfekt! Hier kommen die fertigen Starter-Scripts fÃ¼r deinen ersten Prototyp:
ðŸš€ analyze_link.py (Kernmodul)
python

import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urlparse
import whois
import tldextract
from datetime import datetime
import google.generativeai as genai
import os
from dotenv import load_dotenv
import re

load_dotenv()

class LinkAuditor:
    def __init__(self):
        self.setup_ai()
        
    def setup_ai(self):
        """Initialize AI models"""
        api_key = os.getenv('GOOGLE_API_KEY')
        if api_key:
            genai.configure(api_key=api_key)
            self.gemini = genai.GenerativeModel('gemini-pro')
        else:
            self.gemini = None
            
    def analyze_url(self, url):
        """Main analysis function - returns complete audit report"""
        print(f"ðŸ” Analysiere: {url}")
        
        # Step 1: Crawl and extract content
        crawl_data = self._crawl_url(url)
        if not crawl_data['success']:
            return {'error': crawl_data['error']}
            
        # Step 2: AI Content Analysis
        ai_analysis = self._analyze_content_ai(
            crawl_data['title'], 
            crawl_data['description']
        )
        
        # Step 3: Technical Analysis
        tech_analysis = self._analyze_technical(url, crawl_data)
        
        # Step 4: Domain Reputation
        domain_analysis = self._analyze_domain(url)
        
        # Combine all results
        return {
            **crawl_data,
            **ai_analysis,
            **tech_analysis,
            **domain_analysis,
            'audit_timestamp': datetime.now().isoformat(),
            'url_analyzed': url
        }
    
    def _crawl_url(self, url):
        """Crawl URL and extract metadata"""
        try:
            headers = {
                'User-Agent': 'LinkAudit-Bot/1.0 (+https://github.com/linkaudit)'
            }
            
            response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract core metadata
            title = soup.find('title')
            title_text = title.get_text().strip() if title else ""
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '').strip() if meta_desc else ""
            
            # Extract Open Graph tags
            og_title = soup.find('meta', property='og:title')
            og_title = og_title.get('content', '').strip() if og_title else ""
            
            og_desc = soup.find('meta', property='og:description') 
            og_desc = og_desc.get('content', '').strip() if og_desc else ""
            
            # Use OG tags if available
            final_title = og_title or title_text
            final_desc = og_desc or description
            
            return {
                'success': True,
                'title': final_title,
                'description': final_desc,
                'final_url': response.url,
                'redirect_count': len(response.history),
                'redirect_chain': [str(r.url) for r in response.history] + [response.url],
                'status_code': response.status_code,
                'content_size_kb': len(response.content) // 1024,
                'meta_keywords': self._extract_meta_keywords(soup)
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _analyze_content_ai(self, title, description):
        """AI-powered content analysis"""
        if not self.gemini:
            return self._fallback_analysis(title, description)
            
        try:
            prompt = self._create_analysis_prompt(title, description)
            response = self.gemini.generate_content(prompt)
            
            # Parse JSON from response
            return self._parse_ai_response(response.text)
            
        except Exception as e:
            print(f"AI-Analyse fehlgeschlagen: {e}")
            return self._fallback_analysis(title, description)
    
    def _create_analysis_prompt(self, title, description):
        """Create optimized prompt for AI analysis"""
        return f"""
        ANALYSEAUFTRAG: Link-Bewertung fÃ¼r Transparenz-Tool
        
        INHALT:
        Titel: {title}
        Beschreibung: {description}
        
        BEWERTUNGSKRITERIEN (1-10 Punkte):
        - EmotionalitÃ¤t: Wie emotional/neutral ist der Inhalt?
        - VertrauenswÃ¼rdigkeit: Wirkt der Inhalt seriÃ¶s/glaubwÃ¼rdig?
        - Polarisierung: Spaltet der Inhalt oder verbindet er?
        - ObjektivitÃ¤t: Faktenbasiert oder meinungsstark?
        - ViralitÃ¤t: Wie leicht verbreitet sich dieser Inhalt?
        - Clickbait-Potential: Sensationalistisch oder sachlich?
        
        ANTWORTE ALS JSON:
        {{
            "emotional_intensity": 0-10,
            "trust_score": 0-10, 
            "polarization_score": 0-10,
            "objectivity_score": 0-10,
            "virality_score": 0-10,
            "clickbait_score": 0-10,
            "summary_de": "2-3 SÃ¤tze Zusammenfassung auf Deutsch",
            "framing_analysis": "Welche Framing-Techniken werden verwendet?",
            "risk_level": "niedrig|mittel|hoch"
        }}
        
        Nur JSON zurÃ¼ckgeben, kein zusÃ¤tzlicher Text.
        """
    
    def _parse_ai_response(self, response_text):
        """Parse AI response and extract JSON"""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
            
        return self._fallback_analysis("", "")
    
    def _fallback_analysis(self, title, description):
        """Fallback analysis when AI fails"""
        text = f"{title} {description}".lower()
        
        # Simple heuristic analysis
        emotional_words = ['schockierend', 'unglaublich', 'sensationell', 'krass', 'wahnsinnig']
        trust_indicators = ['studie', 'forschung', 'experte', 'wissenschaftlich']
        clickbait_indicators = ['das wird dich verblÃ¼ffen', 'nummer 7 ist unglaublich', 'was dann passierte']
        
        emotional_score = 3 + sum(1 for word in emotional_words if word in text)
        trust_score = 5 + sum(1 for word in trust_indicators if word in text)
        clickbait_score = sum(1 for phrase in clickbait_indicators if phrase in text) * 2
        
        return {
            'emotional_intensity': min(emotional_score, 10),
            'trust_score': min(trust_score, 10),
            'polarization_score': 5,
            'objectivity_score': 7,
            'virality_score': 6,
            'clickbait_score': min(clickbait_score, 10),
            'summary_de': 'Basanalyse: Inhalt konnte automatisch bewertet werden',
            'framing_analysis': 'Automatische Analyse - fÃ¼r detaillierte Bewertung KI aktivieren',
            'risk_level': 'niedrig'
        }
    
    def _analyze_technical(self, url, crawl_data):
        """Technical URL analysis"""
        parsed = urlparse(url)
        
        # Tracking parameter detection
        tracking_params = ['utm_', 'fbclid', 'gclid', 'msclkid', 'trk_', 'ref_', 'source']
        query_params = parsed.query
        found_tracking = []
        
        if query_params:
            for param in query_params.split('&'):
                if any(indicator in param.lower() for indicator in tracking_params):
                    found_tracking.append(param)
        
        # Security indicators
        security_score = 5
        if parsed.scheme == 'https':
            security_score += 3
        
        return {
            'tracking_params_found': found_tracking,
            'tracking_param_count': len(found_tracking),
            'security_score': security_score,
            'has_https': parsed.scheme == 'https',
            'parameter_count': len(query_params.split('&')) if query_params else 0
        }
    
    def _analyze_domain(self, url):
        """Domain reputation analysis"""
        try:
            extracted = tldextract.extract(url)
            domain = f"{extracted.domain}.{extracted.suffix}"
            
            # WHOIS lookup
            whois_info = whois.whois(domain)
            
            # Domain age calculation
            creation_date = whois_info.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]
                
            domain_age_days = None
            if creation_date:
                domain_age_days = (datetime.now() - creation_date).days
            
            # Simple reputation scoring
            reputation_score = 5
            if domain_age_days and domain_age_days > 365:  # Older than 1 year
                reputation_score += 2
            if extracted.suffix in ['.org', '.edu', '.gov']:
                reputation_score += 1
            
            return {
                'domain': domain,
                'domain_age_days': domain_age_days,
                'tld': extracted.suffix,
                'registrar': whois_info.registrar,
                'creation_date': str(creation_date) if creation_date else None,
                'reputation_score': min(reputation_score, 10),
                'country': whois_info.country
            }
            
        except Exception as e:
            return {
                'domain': domain,
                'error': f"Domain-Analyse fehlgeschlagen: {str(e)}",
                'reputation_score': 3
            }
    
    def _extract_meta_keywords(self, soup):
        """Extract meta keywords if available"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords and meta_keywords.get('content'):
            return [kw.strip() for kw in meta_keywords.get('content').split(',')]
        return []

# Usage example
if __name__ == "__main__":
    auditor = LinkAuditor()
    
    # Test URL
    test_url = "https://example.com"
    result = auditor.analyze_url(test_url)
    
    print("ðŸ”— LINKAUDIT ERGEBNIS")
    print("=" * 50)
    print(f"URL: {result.get('url_analyzed')}")
    print(f"Titel: {result.get('title')}")
    print(f"Vertrauens-Score: {result.get('trust_score')}/10")
    print(f"Zusammenfassung: {result.get('summary_de')}")
    print(f"Risk-Level: {result.get('risk_level')}")

ðŸ“‹ launch_backlog.py (Priorisierte Aufgaben)
python

LAUNCH_BACKLOG = {
    "PHASE 1: MVP (Woche 1)": [
        {
            "id": "MVP-1",
            "task": "Basis-Streamlit UI implementieren",
            "priority": "critical",
            "estimate": "4 hours",
            "status": "ready",
            "description": "Einfache OberflÃ¤che mit URL-Eingabe und Ergebnisanzeige"
        },
        {
            "id": "MVP-2", 
            "task": "Core Analysis Module integrieren",
            "priority": "critical",
            "estimate": "3 hours",
            "status": "ready",
            "description": "LinkAuditor Klasse in Streamlit einbinden"
        },
        {
            "id": "MVP-3",
            "task": "Ergebnis-Visualisierung (Radar-Chart)",
            "priority": "high",
            "estimate": "2 hours", 
            "status": "ready",
            "description": "Plotly Radar-Chart fÃ¼r Impact-Analyse"
        },
        {
            "id": "MVP-4",
            "task": ".env Konfiguration dokumentieren",
            "priority": "high",
            "estimate": "1 hour",
            "status": "ready",
            "description": "API Keys und Setup-Anleitung"
        }
    ],
    
    "PHASE 2: Enhancement (Woche 2)": [
        {
            "id": "ENH-1",
            "task": "Export-Funktionen (JSON, PDF)",
            "priority": "high",
            "estimate": "3 hours",
            "status": "planned",
            "description": "Ergebnis-Export fÃ¼r Berichte"
        },
        {
            "id": "ENH-2",
            "task": "Caching implementieren",
            "priority": "medium", 
            "estimate": "2 hours",
            "status": "planned",
            "description": "Doppelanalysen vermeiden"
        },
        {
            "id": "ENH-3",
            "task": "Error Handling verbessern",
            "priority": "medium",
            "estimate": "2 hours",
            "status": "planned",
            "description": "Robustere Fehlerbehandlung"
        }
    ],
    
    "PHASE 3: Scaling (Woche 3+)": [
        {
            "id": "SCALE-1",
            "task": "OpenPageRank API Integration",
            "priority": "medium",
            "estimate": "2 hours",
            "status": "backlog",
            "description": "Domain Authority Scores"
        },
        {
            "id": "SCALE-2",
            "task": "Multi-KI Fallback (OpenAI + Gemini)",
            "priority": "low",
            "estimate": "3 hours",
            "status": "backlog", 
            "description": "Redundanz fÃ¼r AI-Analyse"
        },
        {
            "id": "SCALE-3",
            "task": "Batch-Analyse fÃ¼r mehrere URLs",
            "priority": "low",
            "estimate": "4 hours",
            "status": "backlog",
            "description": "Vergleichende Analyse"
        }
    ]
}

def print_backlog():
    print("ðŸš€ LINKAUDIT LAUNCH BACKLOG")
    print("=" * 60)
    
    for phase, tasks in LAUNCH_BACKLOG.items():
        print(f"\n{phase}")
        print("-" * 40)
        
        for task in tasks:
            status_icon = "âœ…" if task["status"] == "done" else "ðŸŸ¡" if task["status"] == "in progress" else "â³"
            priority_icon = "ðŸ”´" if task["priority"] == "critical" else "ðŸŸ " if task["priority"] == "high" else "ðŸ”µ"
            
            print(f"{status_icon} {priority_icon} {task['id']}: {task['task']}")
            print(f"   ðŸ“… {task['estimate']} | {task['description']}")

if __name__ == "__main__":
    print_backlog()

ðŸŽ¯ prompt_templates.py (KI-Prompt-Optimierung)
python

PROMPT_TEMPLATES = {
    "content_analysis": {
        "system": """Du bist ein neutraler Medien-Analyst. Deine Aufgabe ist die objektive Bewertung von Web-Inhalten hinsichtlich Transparenz, VertrauenswÃ¼rdigkeit und Wirkung.""",
        
        "analysis_framework": """
        ANALYSE-RAHMEN:
        
        1. EMOTIONALE WIRKUNG (1-10)
        - 1-3: Sehr neutral/faktisch
        - 4-6: Leicht emotional
        - 7-8: Emotional/Ã¼berzeugend  
        - 9-10: Hoch emotional/sensationell
        
        2. VERTRAUENSWÃœRDIGKEIT (1-10)
        - 1-3: Sehr fragwÃ¼rdig
        - 4-6: Neutral/nicht bewertbar
        - 7-8: SeriÃ¶s/glaubwÃ¼rdig
        - 9-10: Sehr vertrauenswÃ¼rdig
        
        3. POLARISIERUNG (1-10)
        - 1-3: Verbindend/ausgleichend
        - 4-6: Neutral
        - 7-8: Leicht polarisierend
        - 9-10: Stark polarisierend/spaltend
        
        4. FRAMING-TECHNIKEN identifizieren:
        - Emotional Framing
        - Fear Mongering
        - Clickbait/Sensationalism
        - Authority Appeal
        - Bandwagon Effect
        - Objective/Neutral
        """
    },
    
    "risk_assessment": {
        "high_risk_indicators": [
            "Extreme emotionale Sprache",
            "Fehlende Quellenangaben", 
            "Sensationelle Behauptungen ohne Belege",
            "Deutliche politische/ideologische Einseitigkeit",
            "VerschwÃ¶rungserzÃ¤hlungen",
            "Hassrede/Diskriminierung"
        ],
        
        "trust_indicators": [
            "Quellen werden genannt",
            "Ausgewogene Darstellung",
            "Faktenbasierte Argumentation",
            "Transparente Autorenschaft",
            "KorrekturmÃ¶glichkeiten/Kontakt"
        ]
    },
    
    "response_format": {
        "json_schema": {
            "emotional_intensity": "number 1-10",
            "trust_score": "number 1-10",
            "polarization_score": "number 1-10", 
            "objectivity_score": "number 1-10",
            "virality_score": "number 1-10",
            "clickbait_score": "number 1-10",
            "summary_de": "string (2-3 SÃ¤tze)",
            "framing_analysis": "string (Framing-Techniken)",
            "risk_level": "string (niedrig|mittel|hoch)",
            "confidence_score": "number 1-10"
        },
        
        "examples": {
            "neutral_news": {
                "emotional_intensity": 3,
                "trust_score": 8,
                "polarization_score": 2,
                "summary_de": "Neutrale, faktenbasierte Berichterstattung mit klarer Quellenangabe."
            },
            "clickbait_content": {
                "emotional_intensity": 9, 
                "trust_score": 3,
                "clickbait_score": 8,
                "summary_de": "Stark emotionalisierter Inhalt mit sensationalistischen Elementen."
            }
        }
    }
}

def get_optimized_prompt(title, description):
    """Generate optimized prompt for content analysis"""
    template = PROMPT_TEMPLATES["content_analysis"]
    
    return f"""
    {template['system']}
    
    ANALYSIERE DIESEN INHALT:
    Titel: {title}
    Beschreibung: {description}
    
    {template['analysis_framework']}
    
    ANTWORTE STRENG NACH DIESEM JSON-FORMAT:
    {PROMPT_TEMPLATES['response_format']['json_schema']}
    
    Beispiele fÃ¼r Bewertungen:
    - Neutrale Nachricht: emotion=3, trust=8, polarization=2  
    - Clickbait: emotion=9, trust=3, clickbait=8
    - Polarisierend: polarization=9, objectivity=2
    
    Nur reines JSON zurÃ¼ckgeben, keine zusÃ¤tzlichen ErklÃ¤rungen.
    """

ðŸš€ Starter Command
bash

# 1. Environment setup
python -m venv linkaudit
source linkaudit/bin/activate  # Windows: .\linkaudit\Scripts\activate

# 2. Install dependencies
pip install streamlit requests beautifulsoup4 google-generativeai python-whois tldextract plotly pdfkit python-dotenv

# 3. Create .env file
echo "GOOGLE_API_KEY=dein_api_key_hier" > .env

# 4. Test the analyzer
python analyze_link.py

# 5. Check launch plan
python launch_backlog.py

# 6. Start Streamlit UI (nach Implementierung)
# streamlit run app.py

Du bist jetzt startklar!

    ðŸŽ¯ analyze_link.py - Kernlogik funktioniert

    ðŸ“‹ launch_backlog.py - Klarer Fahrplan

    ðŸŽ¨ prompt_templates.py - Optimierte KI-Prompts

    ðŸ”§ requirements.txt - Alle AbhÃ¤ngigkeiten

NÃ¤chster Schritt: Streamlit UI implementieren (ca. 2-3 Stunden) und du hast einen voll funktionsfÃ¤higen Prototypen!